# Google发布Gemini 3.1 Pro，核心推理能力大幅提升

今天早上，Google DeepMind悄然发布了Gemini 3.1 Pro。没有发布会，没有预告片，直接在官网挂出了最新成绩单。我点进去一看，好家伙，这次确实有点东西。

**先看最核心的推理能力。**

Gemini 3.1 Pro在「人类最后考试」(Humanity's Last Exam)上拿到44.4%的准确率。这个考试被业内称为「AI炼金石」，专门考验模型在没有任何工具辅助下的真实推理能力。配合搜索和代码执行，准确率可以拉到51.4%。

更夸张的是代码能力。在Codeforces编程竞赛中，Gemini 3.1 Pro达到了2887 Elo。这是什么概念？差不多相当于一个认真备赛两年的竞赛选手水平。SWE-Bench Verified（软件工程基准）上，它一次能解决80.6%的真实世界编程问题。

**但我觉得最恐怖的，是它的Agent能力。**

你们知道现在各大厂商都在卷什么吗？不是单纯的对答，而是「让AI帮你干活」。Gemini 3.1 Pro的Agent能力，在Terminal-Bench 2.0上拿到68.5%，在τ2-bench（工具使用基准）上，零售场景90.8%，电信场景更是高达99.3%。

换句话说，它不仅能回答问题，还能自己调用工具、连续执行多步操作、帮你完成复杂的工作流。

**再说一个容易被忽略的点：多模态和长文本。**

MMMU-Pro（多模态理解）上80.5%，MMMLU（多语言问答）上92.6%。长文本测试MRCR v2里，128k上下文长度下84.9%的准确率。这意味着什么？你丢给它一篇几万字的论文，它能准确理解并回答问题。

当然，Google还藏了一手：Gemini 3 Deep Think。

这是专门为复杂推理打造的增强模式，在ARC-AGI-2（抽象推理）上达到84.6%，在国际数学奥林匹克2025上拿到81.5%，物理87.7%，化学82.8%。不过这个模式目前只开放给Google AI Ultra订阅用户。

**我的观点：**

这次Gemini 3.1 Pro的核心策略很清楚了——不做全能选手，而是死磕推理和Agent。

你们发现没有，Google不再强调「我们模型多大、参数多少」，而是直接告诉你「能帮你解决什么问题」。从Deep Think模式的命名也能看出来，它想让模型像人一样「深度思考」，而不是单纯堆算力。

对于我们这些做AI工具、AI编程的人来说，这是个好消息。模型越靠谱，我们做产品的时候越有底气。但反过来想，如果AI都能自己调用工具、自己完成工作流，那Prompt Engineer的价值在哪里？Agent设计师的价值又在哪里？

这个问题值得好好想想。

---

**你们觉得 Gemini 3.1 Pro 这波能打几分？推理能力真的「封神」了吗？**

---

*参考资料：*
- Google DeepMind 官方模型页面
- Humanity's Last Exam 基准测试
- Codeforces 编程竞赛排名
- SWE-Bench Verified 软件工程基准
