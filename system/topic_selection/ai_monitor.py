#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIçƒ­ç‚¹ç›‘æ§ç³»ç»Ÿ
æ¯æ—¥è‡ªåŠ¨æŠ“å–AIè¡Œä¸šåŠ¨æ€ï¼Œç”Ÿæˆå€™é€‰ä¸»é¢˜æŠ¥å‘Š

è¿è¡Œæ–¹å¼ï¼š
    python ai_monitor.py              # è¿è¡Œä¸€æ¬¡
    python ai_monitor.py --daemon     # åå°è¿è¡Œï¼ˆæ¯4å°æ—¶ï¼‰
    python ai_monitor.py --test       # æµ‹è¯•æ¨¡å¼ï¼ˆä»…HackerNewsï¼‰
"""

import requests
import feedparser
import json
from datetime import datetime, timedelta
from collections import Counter
from typing import List, Dict
import time
import sys

# ============================================
# é…ç½®
# ============================================

# Brave Search APIé…ç½®ï¼ˆå¯é€‰ï¼‰
BRAVE_API_KEY = ""  # ä» https://brave.com/search/api/ è·å–

# å…³é”®è¯é…ç½®
AI_KEYWORDS = [
    "AI", "artificial intelligence", "machine learning", "deep learning",
    "GPT", "Gemini", "Claude", "LLM", "large language model",
    "generative AI", "AI tools", "AI coding", "AI startup",
    "OpenAI", "Anthropic", "Google AI", "neural network"
]

# çƒ­åº¦é˜ˆå€¼
TRENDING_THRESHOLD = {
    "high": 5000,    # 24å°æ—¶å†…è®¨è®ºé‡ > 5000
    "medium": 1000,  # 1000-5000
    "low": 100       # 100-1000
}

# ============================================
# æ•°æ®é‡‡é›†æ¨¡å—
# ============================================

def fetch_hackernews():
    """æŠ“å–HackerNewsçƒ­é—¨è¯é¢˜"""
    print("ğŸ“° æ­£åœ¨æŠ“å– HackerNews...")
    
    try:
        url = "https://hacker-news.firebaseio.com/v0/topstories.json"
        story_ids = requests.get(url, timeout=10).json()[:30]
        
        stories = []
        for story_id in story_ids:
            story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            story = requests.get(story_url, timeout=10).json()
            
            if story and 'title' in story:
                title = story['title'].lower()
                if any(kw.lower() in title for kw in AI_KEYWORDS):
                    stories.append({
                        "source": "HackerNews",
                        "title": story['title'],
                        "url": story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                        "score": story.get('score', 0),
                        "comments": story.get('descendants', 0),
                        "timestamp": datetime.now().isoformat()
                    })
        
        print(f"  âœ… æ‰¾åˆ° {len(stories)} æ¡AIç›¸å…³è¯é¢˜")
        return stories
    
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥ï¼š{e}")
        return []

def fetch_brave_search():
    """ä½¿ç”¨Brave Search APIæœç´¢AIçƒ­ç‚¹"""
    print("ğŸ” æ­£åœ¨ä½¿ç”¨ Brave Search...")
    
    if not BRAVE_API_KEY or BRAVE_API_KEY == "":
        print("  âš ï¸  æœªé…ç½®Brave API Keyï¼Œè·³è¿‡")
        return []
    
    try:
        headers = {
            "Accept": "application/json",
            "X-Subscription-Token": BRAVE_API_KEY
        }
        
        results = []
        queries = ["AI news today", "new AI tools 2026", "AI startup"]
        
        for query in queries:
            url = f"https://api.search.brave.com/res/v1/web/search?q={query}&count=10"
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                for item in data.get('web', {}).get('results', []):
                    results.append({
                        "source": "Brave Search",
                        "title": item.get('title'),
                        "url": item.get('url'),
                        "description": item.get('description'),
                        "timestamp": datetime.now().isoformat()
                    })
        
        print(f"  âœ… æ‰¾åˆ° {len(results)} æ¡æœç´¢ç»“æœ")
        return results
    
    except Exception as e:
        print(f"  âŒ æœç´¢å¤±è´¥ï¼š{e}")
        return []

def fetch_producthunt():
    """æŠ“å–ProductHunt AIå·¥å…·"""
    print("ğŸš€ æ­£åœ¨æŠ“å– ProductHunt...")
    
    try:
        feed = feedparser.parse("https://www.producthunt.com/topics/artificial-intelligence.rss")
        
        tools = []
        for entry in feed.entries[:10]:
            tools.append({
                "source": "ProductHunt",
                "title": entry.title,
                "url": entry.link,
                "description": entry.get('summary', ''),
                "timestamp": entry.get('published', datetime.now().isoformat())
            })
        
        print(f"  âœ… æ‰¾åˆ° {len(tools)} ä¸ªAIå·¥å…·")
        return tools
    
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥ï¼š{e}")
        return []

def fetch_arxiv():
    """æŠ“å–arXiv AIè®ºæ–‡"""
    print("ğŸ“š æ­£åœ¨æŠ“å– arXiv...")
    
    try:
        feeds = [
            "http://export.arxiv.org/rss/cs.AI",
            "http://export.arxiv.org/rss/cs.CL",
            "http://export.arxiv.org/rss/cs.LG"
        ]
        
        papers = []
        for feed_url in feeds:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries[:5]:
                papers.append({
                    "source": "arXiv",
                    "title": entry.title,
                    "url": entry.link,
                    "description": entry.get('summary', ''),
                    "timestamp": entry.get('published', datetime.now().isoformat())
                })
        
        print(f"  âœ… æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥ï¼š{e}")
        return []

def fetch_techcrunch():
    """æŠ“å–TechCrunch AIæ–°é—»"""
    print("ğŸ“° æ­£åœ¨æŠ“å– TechCrunch...")
    
    try:
        feed = feedparser.parse("https://techcrunch.com/category/artificial-intelligence/feed/")
        
        news = []
        for entry in feed.entries[:10]:
            news.append({
                "source": "TechCrunch",
                "title": entry.title,
                "url": entry.link,
                "description": entry.get('summary', ''),
                "timestamp": entry.get('published', datetime.now().isoformat())
            })
        
        print(f"  âœ… æ‰¾åˆ° {len(news)} æ¡æ–°é—»")
        return news
    
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥ï¼š{e}")
        return []

# ============================================
# æ•°æ®å¤„ç†æ¨¡å—
# ============================================

def calculate_heat(item: Dict) -> int:
    """è®¡ç®—çƒ­åº¦åˆ†æ•°"""
    heat = 0
    
    # HackerNewsçƒ­åº¦
    if item['source'] == 'HackerNews':
        heat = item.get('score', 0) + item.get('comments', 0) * 2
    
    # å…¶ä»–æ¥æºåŸºç¡€åˆ†
    else:
        heat = 100
    
    # æ—¶æ•ˆæ€§åŠ æˆ
    try:
        timestamp = datetime.fromisoformat(item['timestamp'].replace('Z', '+00:00'))
        hours_ago = (datetime.now() - timestamp.replace(tzinfo=None)).total_seconds() / 3600
        if hours_ago < 24:
            heat *= 2
        elif hours_ago < 72:
            heat *= 1.5
    except:
        pass
    
    return int(heat)

def classify_topic(title: str, description: str = "") -> List[str]:
    """åˆ†ç±»åˆ°å†…å®¹çº¿"""
    text = (title + " " + description).lower()
    
    categories = []
    
    # AIç§‘æ™®å…³é”®è¯
    if any(kw in text for kw in ['explain', 'understand', 'what is', 'how does', 'introduction', 'guide']):
        categories.append("AIç§‘æ™®")
    
    # AIå·¥å…·å…³é”®è¯
    if any(kw in text for kw in ['tool', 'app', 'software', 'platform', 'api', 'product', 'launch']):
        categories.append("AIå·¥å…·")
    
    # AIç¼–ç¨‹å…³é”®è¯
    if any(kw in text for kw in ['code', 'coding', 'programming', 'developer', 'github', 'open source', 'library']):
        categories.append("AIç¼–ç¨‹")
    
    # AIå‡ºæµ·åˆ›ä¸šå…³é”®è¯
    if any(kw in text for kw in ['startup', 'funding', 'market', 'business', 'revenue', 'valuation', 'raises']):
        categories.append("AIå‡ºæµ·åˆ›ä¸š")
    
    # é»˜è®¤åˆ†ç±»
    if not categories:
        categories.append("AIç§‘æ™®")
    
    return categories

def deduplicate(items: List[Dict]) -> List[Dict]:
    """å»é‡"""
    seen_titles = set()
    unique_items = []
    
    for item in items:
        # ç®€å•å»é‡ï¼šæ¯”è¾ƒæ ‡é¢˜å‰50ä¸ªå­—ç¬¦
        title_key = item['title'][:50].lower()
        if title_key not in seen_titles:
            seen_titles.add(title_key)
            unique_items.append(item)
    
    return unique_items

# ============================================
# æŠ¥å‘Šç”Ÿæˆæ¨¡å—
# ============================================

def generate_report(items: List[Dict]) -> str:
    """ç”ŸæˆMarkdownæŠ¥å‘Š"""
    
    # æŒ‰çƒ­åº¦æ’åº
    items_with_heat = []
    for item in items:
        item['heat'] = calculate_heat(item)
        item['categories'] = classify_topic(item['title'], item.get('description', ''))
        items_with_heat.append(item)
    
    items_sorted = sorted(items_with_heat, key=lambda x: x['heat'], reverse=True)
    
    # åˆ†çº§
    high_heat = [x for x in items_sorted if x['heat'] >= TRENDING_THRESHOLD['high']]
    medium_heat = [x for x in items_sorted if TRENDING_THRESHOLD['medium'] <= x['heat'] < TRENDING_THRESHOLD['high']]
    low_heat = [x for x in items_sorted if TRENDING_THRESHOLD['low'] <= x['heat'] < TRENDING_THRESHOLD['medium']]
    
    # ç”ŸæˆæŠ¥å‘Š
    report = []
    report.append(f"# AIçƒ­ç‚¹æ—¥æŠ¥")
    report.append(f"")
    report.append(f"> ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"> æ•°æ®æºï¼šHackerNews, ProductHunt, arXiv, TechCrunch, Brave Search")
    report.append(f"> æ€»è®¡ï¼š{len(items_sorted)} æ¡çƒ­ç‚¹")
    report.append(f"")
    report.append(f"---")
    report.append(f"")
    
    # é«˜çƒ­åº¦
    if high_heat:
        report.append(f"## ğŸ”¥ğŸ”¥ğŸ”¥ é«˜çƒ­åº¦ï¼ˆ{len(high_heat)}æ¡ï¼‰")
        report.append(f"")
        for i, item in enumerate(high_heat, 1):
            report.append(f"### {i}. {item['title']}")
            report.append(f"")
            report.append(f"- **æ¥æº**ï¼š{item['source']}")
            report.append(f"- **çƒ­åº¦**ï¼š{item['heat']}")
            report.append(f"- **æ¨èå†…å®¹çº¿**ï¼š{', '.join(item['categories'])}")
            report.append(f"- **é“¾æ¥**ï¼š{item['url']}")
            if item.get('description'):
                desc = item['description'][:200].strip()
                if desc:
                    report.append(f"- **ç®€ä»‹**ï¼š{desc}...")
            report.append(f"")
    else:
        report.append(f"## ğŸ”¥ğŸ”¥ğŸ”¥ é«˜çƒ­åº¦ï¼ˆ0æ¡ï¼‰")
        report.append(f"")
        report.append(f"*ä»Šæ—¥æš‚æ— é«˜çƒ­åº¦è¯é¢˜*")
        report.append(f"")
    
    # ä¸­çƒ­åº¦
    if medium_heat:
        report.append(f"## ğŸ”¥ ä¸­çƒ­åº¦ï¼ˆ{len(medium_heat)}æ¡ï¼‰")
        report.append(f"")
        for i, item in enumerate(medium_heat, 1):
            report.append(f"{i}. **{item['title']}** ({item['source']}, çƒ­åº¦{item['heat']})")
            report.append(f"   - æ¨èï¼š{', '.join(item['categories'])}")
            report.append(f"   - {item['url']}")
            report.append(f"")
    
    # ä½çƒ­åº¦ï¼ˆä»…åˆ—æ ‡é¢˜ï¼‰
    if low_heat:
        report.append(f"## ğŸ’¤ ä½çƒ­åº¦ï¼ˆ{len(low_heat)}æ¡ï¼‰")
        report.append(f"")
        for item in low_heat[:10]:
            report.append(f"- {item['title']} ({item['source']})")
        if len(low_heat) > 10:
            report.append(f"- ... è¿˜æœ‰ {len(low_heat)-10} æ¡")
        report.append(f"")
    
    # ç»Ÿè®¡
    report.append(f"---")
    report.append(f"")
    report.append(f"## ğŸ“Š ç»Ÿè®¡")
    report.append(f"")
    
    category_counter = Counter()
    for item in items_sorted:
        for cat in item['categories']:
            category_counter[cat] += 1
    
    report.append(f"**å†…å®¹çº¿åˆ†å¸ƒ**ï¼š")
    for cat, count in category_counter.most_common():
        report.append(f"- {cat}: {count}æ¡")
    report.append(f"")
    
    source_counter = Counter([x['source'] for x in items_sorted])
    report.append(f"**æ•°æ®æºåˆ†å¸ƒ**ï¼š")
    for source, count in source_counter.most_common():
        report.append(f"- {source}: {count}æ¡")
    report.append(f"")
    
    # ä¸‹ä¸€æ­¥å»ºè®®
    report.append(f"---")
    report.append(f"")
    report.append(f"## ğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®")
    report.append(f"")
    
    if high_heat:
        report.append(f"âœ… å»ºè®®ç«‹å³å¯¹ä»¥ä¸‹é«˜çƒ­åº¦è¯é¢˜è¿›è¡Œæ·±åº¦è°ƒç ”ï¼š")
        for item in high_heat[:3]:
            report.append(f"- {item['title']}")
        report.append(f"")
        report.append(f"å¯ä½¿ç”¨ `python topic_scorer.py \"ä¸»é¢˜åç§°\"` è¿›è¡Œè¯„åˆ†")
    else:
        report.append(f"âš ï¸  ä»Šæ—¥æ— é«˜çƒ­åº¦è¯é¢˜ï¼Œå»ºè®®ï¼š")
        report.append(f"- è§‚å¯Ÿä¸­çƒ­åº¦è¯é¢˜çš„å‘å±•")
        report.append(f"- æˆ–ä»é€‰é¢˜åº“ä¸­é€‰æ‹©å¸¸é’é€‰é¢˜")
    
    return "\n".join(report)

# ============================================
# ä¸»ç¨‹åº
# ============================================

def run_monitor(test_mode=False):
    """è¿è¡Œç›‘æ§"""
    print(f"\n{'='*60}")
    print(f"ğŸ¤– AIçƒ­ç‚¹ç›‘æ§ç³»ç»Ÿ")
    print(f"{'='*60}\n")
    
    all_items = []
    
    # é‡‡é›†æ•°æ®
    if test_mode:
        print("âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šä»…æŠ“å–HackerNews\n")
        all_items.extend(fetch_hackernews())
    else:
        all_items.extend(fetch_hackernews())
        all_items.extend(fetch_brave_search())
        all_items.extend(fetch_producthunt())
        all_items.extend(fetch_arxiv())
        all_items.extend(fetch_techcrunch())
    
    # å»é‡
    all_items = deduplicate(all_items)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æ•°æ®é‡‡é›†å®Œæˆï¼šå…± {len(all_items)} æ¡ï¼ˆå»é‡åï¼‰")
    print(f"{'='*60}\n")
    
    if len(all_items) == 0:
        print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•çƒ­ç‚¹ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å…³é”®è¯é…ç½®")
        return None, None
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_report(all_items)
    
    # ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_file = f"ai_trending_{timestamp}.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆï¼š{report_file}\n")
    
    # ä¿å­˜JSON
    json_file = f"ai_trending_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(all_items, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ•°æ®å·²ä¿å­˜ï¼š{json_file}\n")
    
    return report_file, json_file

def main():
    """ä¸»å‡½æ•°"""
    test_mode = "--test" in sys.argv
    daemon_mode = "--daemon" in sys.argv
    
    if daemon_mode:
        print("ğŸ”„ åå°è¿è¡Œæ¨¡å¼ï¼ˆæ¯4å°æ—¶æ‰§è¡Œä¸€æ¬¡ï¼‰")
        print("æŒ‰ Ctrl+C åœæ­¢\n")
        
        while True:
            try:
                run_monitor(test_mode)
                next_run = datetime.now() + timedelta(hours=4)
                print(f"\nâ° ä¸‹æ¬¡è¿è¡Œæ—¶é—´ï¼š{next_run.strftime('%Y-%m-%d %H:%M:%S')}")
                print(f"â³ ç­‰å¾…ä¸­...\n")
                time.sleep(4 * 3600)
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç›‘æ§å·²åœæ­¢")
                break
            except Exception as e:
                print(f"\nâŒ è¿è¡Œå‡ºé”™ï¼š{e}")
                print(f"â° 5åˆ†é’Ÿåé‡è¯•...\n")
                time.sleep(300)
    else:
        run_monitor(test_mode)

if __name__ == "__main__":
    main()
